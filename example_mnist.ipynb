{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on MNIST\n",
    "\n",
    "This example illustrates how ZerO works and avoids the training degeneracy (described by Thereom 1 in the paper). \n",
    "\n",
    "Link of the paper: https://arxiv.org/abs/2110.12661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import hadamard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We consider a 4-layer multi-layer perceptron (MLP) where the hidden dimension is fixed. The models based on **random, partial identity, and ZerO** initialization are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZerO_Init_on_matrix(matrix_tensor):\n",
    "    # Algorithm 1 in the paper.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    \n",
    "    if m <= n:\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    elif m > n:\n",
    "        clog_m = math.ceil(math.log2(m))\n",
    "        p = 2**(clog_m)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, p))\\\n",
    "                    @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2)))\\\n",
    "                    @ torch.nn.init.eye_(torch.empty(p, n))\\\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def Identity_Init_on_matrix(matrix_tensor):\n",
    "    # Definition 1 in the paper\n",
    "    # See https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_ for details. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible, the same as partial identity matrix.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    print(\"Init matrix\", matrix_tensor.shape, \"with m={}, n={}\".format(m, n))\n",
    "    \n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def OnE_Init_on_matrix(matrix_tensor):\n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "    if m <= n:\n",
    "        print(\"Nothing extra to be done to OnE-Initialize\", init_matrix.shape);\n",
    "    elif m > n:\n",
    "        print(\"OnE-Initializing\", init_matrix.shape)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "        rng = np.random.default_rng()\n",
    "        for row in range(n, m, 1):\n",
    "            col = rng.integers(low=0, high=n-1, endpoint=True)\n",
    "            #print(\"Random column selected to be initialized to OnE (0 to {}): {}\".format(n-1, col))\n",
    "            init_matrix[row, col] = 1\n",
    "    else:\n",
    "        assert(False)\n",
    "    return init_matrix\n",
    "            \n",
    "\n",
    "def Spray_Init_on_matrix(matrix_tensor):\n",
    "    \"\"\" Fill like OnE init, but invert effect\"\"\"\n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "    if m <= n:\n",
    "        print(\"Nothing extra to be done to OnE-Initialize\", init_matrix.shape);\n",
    "    elif m > n:\n",
    "        print(\"OnE-Initializing\", init_matrix.shape)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "        rng = np.random.default_rng()\n",
    "        for row in range(n, m, 1):\n",
    "            col = rng.integers(low=0, high=n-1, endpoint=True)\n",
    "            #print(\"Random column selected to be initialized to OnE (0 to {}): {}\".format(n-1, col))\n",
    "            init_matrix[row, col] = 1\n",
    "    else:\n",
    "        assert(False)\n",
    "    # Invert effect\n",
    "    fix_value = 1 / (n-1) + 1.0\n",
    "    helper = torch.nn.init.constant(torch.empty(m,n), fix_value)\n",
    "    init_matrix = helper - init_matrix;\n",
    "    return init_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    a standard model with 4 hidden layers\n",
    "    '''\n",
    "    def __init__(self, n_h=1024, init='ZerO'):\n",
    "        super(MLP, self).__init__()\n",
    "        self.init = init\n",
    "        self.n_h = n_h\n",
    "        self.l1 = nn.Linear(784, 784, bias=False)  \n",
    "        self.l2 = nn.Linear(784, self.n_h, bias=False)  \n",
    "        self.l3 = nn.Linear(self.n_h, self.n_h, bias=False)  \n",
    "        self.l4 = nn.Linear(self.n_h, 10, bias=False)  \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l4(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \n",
    "        if self.init == 'ZerO':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = ZerO_Init_on_matrix(m.weight.data)\n",
    "                \n",
    "        elif self.init == 'Partial_Identity':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = Identity_Init_on_matrix(m.weight.data)\n",
    "        \n",
    "        elif self.init == 'Random':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "        elif self.init == 'OnE':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = OnE_Init_on_matrix(m.weight.data)\n",
    "        elif self.init == 'Spray':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = Spray_Init_on_matrix(m.weight.data)\n",
    "        else:\n",
    "            assert(False)\n",
    "                \n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(tensor):\n",
    "\n",
    "    tensor = tensor.detach().cpu()\n",
    "    rank = np.linalg.matrix_rank(tensor, tol=0.0001)\n",
    "    \n",
    "    return rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline on MNIST\n",
    "\n",
    "from https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Theorem 1 (Figure 3 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(torch.optim.SGD):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                d_p = p.grad.data\n",
    "                p.data.add_(d_p, alpha=-group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy({:.0f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                100. * correct / len(train_loader.dataset)))\n",
    "            \n",
    "            # log metric\n",
    "            train_acc_list.append(100. * correct / len(train_loader.dataset))\n",
    "            train_loss_list.append(loss.item())\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if 'l3' in name:\n",
    "                        if name not in rank_list_dict:\n",
    "                            rank_list_dict[name] = []\n",
    "                            \n",
    "                        # compute stable rank of the residual component\n",
    "                        rank_list_dict[name].append(compute_rank(param.data - torch.eye(param.data.size(0)).to(param.data.device)))\n",
    "                \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train_model(model, file_dir=None):\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "\n",
    "    parser.add_argument('--name', type=str, default='test')  \n",
    "    \n",
    "    parser.add_argument('--init', type=str, default='ZerO') \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = model.to(device)\n",
    "    # debug\n",
    "    print(\"v ===== Before training (init={}) =====\".format(init))\n",
    "    for name, params in model.named_parameters():\n",
    "        print(params.shape)\n",
    "        print(name, params.data)\n",
    "    print(\"^ ===== Before training (init={}) =====\".format(init))   \n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    " \n",
    "    # logging metric    \n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    rank_list_dict = {}\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=12, gamma=args.gamma)\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "    # debug2\n",
    "    print(\"v ===== After training (init={}) =====\".format(init))\n",
    "    for name, params in model.named_parameters():\n",
    "        print(params.shape)\n",
    "        print(name, params.data)\n",
    "    print(\"^ ===== After training (init={}) =====\".format(init))\n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), args.init + \"_mnist_cnn.pt\")\n",
    "        \n",
    "    return rank_list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (left): identity initialization under different widths\n",
    "\n",
    "We show that the rank constraints (training degeneracy) happen no matter what the width is. The ranks are always smaller than the input dimension (784=28 * 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([256, 784]) with m=256, n=784\n",
      "Init matrix torch.Size([256, 256]) with m=256, n=256\n",
      "Init matrix torch.Size([10, 256]) with m=10, n=256\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([256, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([256, 256])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 256])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12879/2007514160.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.973962\tAccuracy(2%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.614879\tAccuracy(6%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.626663\tAccuracy(11%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.337751\tAccuracy(15%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.646007\tAccuracy(20%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.255932\tAccuracy(25%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.715249\tAccuracy(29%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.408288\tAccuracy(34%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.407710\tAccuracy(39%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.141189\tAccuracy(44%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.129136\tAccuracy(49%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.269307\tAccuracy(54%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.133370\tAccuracy(59%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.087320\tAccuracy(64%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.111893\tAccuracy(69%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.283707\tAccuracy(74%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.347303\tAccuracy(79%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.116524\tAccuracy(84%)\n",
      "\n",
      "Test set: Average loss: 0.2090, Accuracy: 9346/10000 (93%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([256, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([256, 256])\n",
      "l3.weight tensor([[1.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 1.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 1.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.0000e+00, 1.0877e-04,\n",
      "         2.7688e-04],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 1.4778e-04, 1.0011e+00,\n",
      "         2.6847e-03],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 3.5949e-04, 2.5675e-03,\n",
      "         1.0065e+00]])\n",
      "torch.Size([10, 256])\n",
      "l4.weight tensor([[ 1.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -1.0436e-03,\n",
      "         -1.1127e-02, -2.8481e-02],\n",
      "        [ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ..., -1.5737e-04,\n",
      "         -2.4953e-03, -7.2980e-03],\n",
      "        [ 0.0000e+00,  0.0000e+00,  1.0000e+00,  ...,  1.1354e-03,\n",
      "          1.6651e-02,  3.8486e-02],\n",
      "        ...,\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  4.8922e-03,\n",
      "          4.0179e-02,  1.0234e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -9.3280e-04,\n",
      "         -5.4381e-03, -1.2592e-02],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -3.2588e-03,\n",
      "         -2.4191e-02, -5.4511e-02]])\n",
      "^ ===== After training =====\n",
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([512, 784]) with m=512, n=784\n",
      "Init matrix torch.Size([512, 512]) with m=512, n=512\n",
      "Init matrix torch.Size([10, 512]) with m=10, n=512\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([512, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([512, 512])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 512])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.424199\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.469808\tAccuracy(8%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.536723\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.266333\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.551432\tAccuracy(22%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.198503\tAccuracy(27%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.679957\tAccuracy(32%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.356694\tAccuracy(37%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.359582\tAccuracy(42%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.138990\tAccuracy(47%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.106483\tAccuracy(52%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.302657\tAccuracy(57%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.097991\tAccuracy(62%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.102183\tAccuracy(67%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.108110\tAccuracy(72%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.207563\tAccuracy(77%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.338353\tAccuracy(82%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.090949\tAccuracy(87%)\n",
      "\n",
      "Test set: Average loss: 0.1751, Accuracy: 9481/10000 (95%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([512, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([512, 512])\n",
      "l3.weight tensor([[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 1.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 1.0129, 0.0089, 0.0031],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0160, 1.0194, 0.0145],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0130, 0.0167, 1.0189]])\n",
      "torch.Size([10, 512])\n",
      "l4.weight tensor([[ 1.0000,  0.0000,  0.0000,  ...,  0.0525,  0.0810,  0.1007],\n",
      "        [ 0.0000,  1.0000,  0.0000,  ..., -0.0450, -0.0799, -0.0878],\n",
      "        [ 0.0000,  0.0000,  1.0000,  ...,  0.1250,  0.1324,  0.1657],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0171, -0.0517, -0.0895],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0799, -0.0566,  0.0055],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ..., -0.0518, -0.0706, -0.0737]])\n",
      "^ ===== After training =====\n",
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([1024, 784]) with m=1024, n=784\n",
      "Init matrix torch.Size([1024, 1024]) with m=1024, n=1024\n",
      "Init matrix torch.Size([10, 1024]) with m=10, n=1024\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.387518\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.401102\tAccuracy(9%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.541363\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.241735\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.560055\tAccuracy(23%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.180239\tAccuracy(27%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.678365\tAccuracy(32%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.327334\tAccuracy(37%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.341815\tAccuracy(42%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.138724\tAccuracy(47%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.099397\tAccuracy(52%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.284145\tAccuracy(57%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.101840\tAccuracy(62%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.110993\tAccuracy(67%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.112484\tAccuracy(72%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.192544\tAccuracy(77%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.324636\tAccuracy(82%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.087515\tAccuracy(87%)\n",
      "\n",
      "Test set: Average loss: 0.1659, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== After training =====\n",
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([2048, 784]) with m=2048, n=784\n",
      "Init matrix torch.Size([2048, 2048]) with m=2048, n=2048\n",
      "Init matrix torch.Size([10, 2048]) with m=10, n=2048\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([2048, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([2048, 2048])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 2048])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.387518\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.401102\tAccuracy(9%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.541363\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.241735\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.559794\tAccuracy(23%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.180234\tAccuracy(27%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.680799\tAccuracy(32%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.326720\tAccuracy(37%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.341754\tAccuracy(42%)\n"
     ]
    }
   ],
   "source": [
    "n_h_256_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=256))\n",
    "n_h_512_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=512))\n",
    "n_h_1024_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "n_h_2048_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=2048))\n",
    "n_h_2048_rank_list_dict_OnE = train_model(MLP(init='OnE', n_h=2048))\n",
    "n_h_2048_rank_list_dict_Spray = train_model(MLP(init='Spray', n_h=2048))\n",
    "\n",
    "\n",
    "# plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, n_h_256_rank_list_dict['l3.weight'], label='n_h=256', linewidth='2')\n",
    "ax.plot(x_axis, n_h_512_rank_list_dict['l3.weight'], label='n_h=512', linewidth='2')\n",
    "ax.plot(x_axis, n_h_1024_rank_list_dict['l3.weight'], label='n_h=1024', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict['l3.weight'], label='n_h=2048', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict_OnE['l3.weight'], label='n_h=2048_OnE', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict_Spray['l3.weight'], label='n_h=2048_Spray', linewidth='2')\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_left.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (right): Hadamard transfrom breaks training degeneracy\n",
    "\n",
    "We show that when initializing dimension-increasing layer with Hadamard transform, the rank constraints (training degeneracy) not exsist any more. The rank can be greater than the input dimension during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_init_rank_list_dict = train_model(MLP(init='Random', n_h=1024))\n",
    "partial_identity_init_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "ZerO_init_rank_list_dict = train_model(MLP(init='ZerO', n_h=1024))\n",
    "OnE_init_rank_list_dict = train_model(MLP(init='OnE', n_h=1024))\n",
    "Spray_init_rank_list_dict = train_model(MLP(init='Spray', n_h=1024))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, random_init_rank_list_dict['l3.weight'], label='Random Init', linewidth='2')\n",
    "ax.plot(x_axis, partial_identity_init_rank_list_dict['l3.weight'], label='Partial Identity Init', linewidth='2')\n",
    "ax.plot(x_axis, ZerO_init_rank_list_dict['l3.weight'], label='ZerO Init', linewidth='2')\n",
    "ax.plot(x_axis, OnE_init_rank_list_dict['l3.weight'], label='OnE Init', linewidth='2')\n",
    "ax.plot(x_axis, Spray_init_rank_list_dict['l3.weight'], label='Spray Init', linewidth='2')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_right.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
