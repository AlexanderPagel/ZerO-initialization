{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on MNIST\n",
    "\n",
    "This example illustrates how ZerO works and avoids the training degeneracy (described by Thereom 1 in the paper). \n",
    "\n",
    "Link of the paper: https://arxiv.org/abs/2110.12661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import hadamard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We consider a 4-layer multi-layer perceptron (MLP) where the hidden dimension is fixed. The models based on **random, partial identity, and ZerO** initialization are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZerO_Init_on_matrix(matrix_tensor):\n",
    "    # Algorithm 1 in the paper.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    \n",
    "    if m <= n:\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    elif m > n:\n",
    "        clog_m = math.ceil(math.log2(m))\n",
    "        p = 2**(clog_m)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, p))\\\n",
    "                    @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2)))\\\n",
    "                    @ torch.nn.init.eye_(torch.empty(p, n))\n",
    "    return init_matrix\n",
    "\n",
    "def More_Init_on_matrix(matrix_tensor):\n",
    "    # Algorithm 1 in the paper.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    \n",
    "    clog_m = math.ceil(math.log2( max(m,n) ))\n",
    "    p = 2**(clog_m)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m, p))\\\n",
    "                @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2)))\\\n",
    "                @ torch.nn.init.eye_(torch.empty(p, n))\n",
    "    if m <= n:\n",
    "        print(\"More initialized\", matrix_tensor.shape, \"to\", init_matrix)\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def Identity_Init_on_matrix(matrix_tensor):\n",
    "    # Definition 1 in the paper\n",
    "    # See https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_ for details. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible, the same as partial identity matrix.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    print(\"Init matrix\", matrix_tensor.shape, \"with m={}, n={}\".format(m, n))\n",
    "    \n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def OnE_Init_on_matrix(matrix_tensor):\n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "    if m <= n:\n",
    "        print(\"Nothing extra to be done to OnE-Initialize\", init_matrix.shape);\n",
    "    elif m > n:\n",
    "        print(\"OnE-Initializing\", init_matrix.shape)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "        rng = np.random.default_rng()\n",
    "        for row in range(n, m, 1):\n",
    "            col = rng.integers(low=0, high=n-1, endpoint=True)\n",
    "            #print(\"Random column selected to be initialized to OnE (0 to {}): {}\".format(n-1, col))\n",
    "            init_matrix[row, col] = 1\n",
    "    else:\n",
    "        assert(False)\n",
    "    return init_matrix\n",
    "            \n",
    "\n",
    "def Spray_Init_on_matrix(matrix_tensor):\n",
    "    \"\"\" Fill like OnE init, but invert effect\"\"\"\n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "    if m <= n:\n",
    "        print(\"Nothing extra to be done to OnE-Initialize\", init_matrix.shape);\n",
    "    elif m > n:\n",
    "        print(\"OnE-Initializing\", init_matrix.shape)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "        rng = np.random.default_rng()\n",
    "        for row in range(n, m, 1):\n",
    "            col = rng.integers(low=0, high=n-1, endpoint=True)\n",
    "            #print(\"Random column selected to be initialized to OnE (0 to {}): {}\".format(n-1, col))\n",
    "            init_matrix[row, col] = 1\n",
    "    else:\n",
    "        assert(False)\n",
    "    # Invert effect\n",
    "    fix_value = 1 / (n-1)\n",
    "    print(\"Spray initializing to value {}\".format(fix_value))\n",
    "    init_matrix = fix_value * (torch.ones(m,n) - init_matrix)\n",
    "    return init_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    a standard model with 4 hidden layers\n",
    "    '''\n",
    "    def __init__(self, n_h=1024, init='ZerO'):\n",
    "        super(MLP, self).__init__()\n",
    "        self.init = init\n",
    "        self.n_h = n_h\n",
    "        self.l1 = nn.Linear(784, 784, bias=True)  \n",
    "        self.l2 = nn.Linear(784, self.n_h, bias=True)  \n",
    "        self.l3 = nn.Linear(self.n_h, self.n_h, bias=True)  \n",
    "        self.l4 = nn.Linear(self.n_h, 10, bias=True)  \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l4(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \n",
    "        if self.init == 'ZerO':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = ZerO_Init_on_matrix(m.weight.data)\n",
    "                \n",
    "        elif self.init == 'Partial_Identity':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = Identity_Init_on_matrix(m.weight.data)\n",
    "        \n",
    "        elif self.init == 'Random':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "        elif self.init == 'OnE':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = OnE_Init_on_matrix(m.weight.data)\n",
    "        elif self.init == 'Spray':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = Spray_Init_on_matrix(m.weight.data)\n",
    "        elif self.init == 'More':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = More_Init_on_matrix(m.weight.data)\n",
    "        else:\n",
    "            assert(False)\n",
    "                \n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(tensor):\n",
    "\n",
    "    tensor = tensor.detach().cpu()\n",
    "    rank = np.linalg.matrix_rank(tensor, tol=0.0001)\n",
    "    \n",
    "    return rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline on MNIST\n",
    "\n",
    "from https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer(torch.optim.SGD):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                d_p = p.grad.data\n",
    "                p.data.add_(d_p, alpha=-group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy({:.0f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                100. * correct / len(train_loader.dataset)))\n",
    "            \n",
    "            # log metric\n",
    "            train_acc_list.append(100. * correct / len(train_loader.dataset))\n",
    "            train_loss_list.append(loss.item())\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if 'l3' in name:\n",
    "                        if name not in rank_list_dict:\n",
    "                            rank_list_dict[name] = []\n",
    "                            \n",
    "                        # compute stable rank of the residual component\n",
    "                        rank_list_dict[name].append(compute_rank(param.data - torch.eye(param.data.size(0)).to(param.data.device)))\n",
    "                \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train_model(model, file_dir=None):\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "\n",
    "    parser.add_argument('--name', type=str, default='test')  \n",
    "    \n",
    "    parser.add_argument('--init', type=str, default='ZerO') \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    print(\"Parsed args:\", args)\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = model.to(device)\n",
    "    # debug\n",
    "    print(\" ---------- Now training init={} ----------\".format(args.init))\n",
    "    print(\"v ===== Before training (args.init={}) =====\".format(args.init))\n",
    "    for name, params in model.named_parameters():\n",
    "        if \"bias\" not in name:\n",
    "            print(params.shape)\n",
    "            print(name, params.data)\n",
    "    print(\"^ ===== Before training (args.init={}) =====\".format(args.init))   \n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    " \n",
    "    # logging metric    \n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    rank_list_dict = {}\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=12, gamma=args.gamma)\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "    # debug2\n",
    "    print(\"v ===== After training (args.init={}) =====\".format(args.init))\n",
    "    for name, params in model.named_parameters():\n",
    "        if \"bias\" not in name:\n",
    "            print(params.shape)\n",
    "            print(name, params.data)\n",
    "    print(\"^ ===== After training (args.init={}) =====\".format(args.init))\n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), args.init + \"_mnist_cnn.pt\")\n",
    "        \n",
    "    return rank_list_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Theorem 1 (Figure 3 in the paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (left): identity initialization under different widths\n",
    "\n",
    "We show that the rank constraints (training degeneracy) happen no matter what the width is. The ranks are always smaller than the input dimension (784=28 * 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([256, 784]) with m=256, n=784\n",
      "Init matrix torch.Size([256, 256]) with m=256, n=256\n",
      "Init matrix torch.Size([10, 256]) with m=10, n=256\n",
      "Parsed args: Namespace(batch_size=64, epochs=1, gamma=0.7, init='ZerO', log_interval=50, lr=0.1, name='test', no_cuda=True, save_model=False, seed=1, test_batch_size=1000)\n",
      " ---------- Now training init=ZerO ----------\n",
      "v ===== Before training (args.init=ZerO) =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([256, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([256, 256])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 256])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training (args.init=ZerO) =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2854/766049207.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.365503\tAccuracy(4%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#n_h_2048_rank_list_dict_Spray = train_model(MLP(init='Spray', n_h=2048))\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#n_h_2048_rank_list_dict_OnE = train_model(MLP(init='OnE', n_h=2048))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m n_h_256_rank_list_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPartial_Identity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m n_h_512_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartial_Identity\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m))\n\u001b[1;32m      5\u001b[0m n_h_1024_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartial_Identity\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 135\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, file_dir)\u001b[0m\n\u001b[1;32m    132\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 135\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_acc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank_list_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     test(model, device, test_loader)\n\u001b[1;32m    137\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\u001b[0m\n\u001b[1;32m     21\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnll_loss(output, target)\n\u001b[1;32m     25\u001b[0m pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m, in \u001b[0;36mMLP.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x): \n\u001b[1;32m     18\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m28\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ml2(x)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#n_h_2048_rank_list_dict_Spray = train_model(MLP(init='Spray', n_h=2048))\n",
    "#n_h_2048_rank_list_dict_OnE = train_model(MLP(init='OnE', n_h=2048))\n",
    "n_h_256_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=256))\n",
    "n_h_512_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=512))\n",
    "n_h_1024_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "n_h_2048_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=2048))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, n_h_256_rank_list_dict['l3.weight'], label='n_h=256', linewidth='2')\n",
    "ax.plot(x_axis, n_h_512_rank_list_dict['l3.weight'], label='n_h=512', linewidth='2')\n",
    "ax.plot(x_axis, n_h_1024_rank_list_dict['l3.weight'], label='n_h=1024', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict['l3.weight'], label='n_h=2048', linewidth='2')\n",
    "#ax.plot(x_axis, n_h_2048_rank_list_dict_OnE['l3.weight'], label='n_h=2048_OnE', linewidth='2')\n",
    "#ax.plot(x_axis, n_h_2048_rank_list_dict_Spray['l3.weight'], label='n_h=2048_Spray', linewidth='2')\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_left.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (right): Hadamard transfrom breaks training degeneracy\n",
    "\n",
    "We show that when initializing dimension-increasing layer with Hadamard transform, the rank constraints (training degeneracy) not exsist any more. The rank can be greater than the input dimension during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2211172251.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    More_init_rank_list_dict = train_modes(MLP(init='More', n_h=1024)):w\u001b[0m\n\u001b[0m                                                                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "More_init_rank_list_dict = train_modes(MLP(init='More', n_h=1024))\n",
    "#Spray_init_rank_list_dict = train_model(MLP(init='Spray', n_h=1024))\n",
    "#OnE_init_rank_list_dict = train_model(MLP(init='OnE', n_h=1024))\n",
    "ZerO_init_rank_list_dict = train_model(MLP(init='ZerO', n_h=1024))\n",
    "partial_identity_init_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "random_init_rank_list_dict = train_model(MLP(init='Random', n_h=1024))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, random_init_rank_list_dict['l3.weight'], label='Random Init', linewidth='2')\n",
    "ax.plot(x_axis, partial_identity_init_rank_list_dict['l3.weight'], label='Partial Identity Init', linewidth='2')\n",
    "ax.plot(x_axis, ZerO_init_rank_list_dict['l3.weight'], label='ZerO Init', linewidth='2')\n",
    "#ax.plot(x_axis, OnE_init_rank_list_dict['l3.weight'], label='OnE Init', linewidth='2')\n",
    "#ax.plot(x_axis, Spray_init_rank_list_dict['l3.weight'], label='Spray Init', linewidth='2')\n",
    "ax.plot(x_axis, More_init_rank_list_dict['l3.weight'], label='More Init', linewidth='2')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_right.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
