{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example on MNIST\n",
    "\n",
    "This example illustrates how ZerO works and avoids the training degeneracy (described by Thereom 1 in the paper). \n",
    "\n",
    "Link of the paper: https://arxiv.org/abs/2110.12661"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import math\n",
    "from scipy.linalg import hadamard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "We consider a 4-layer multi-layer perceptron (MLP) where the hidden dimension is fixed. The models based on **random, partial identity, and ZerO** initialization are defined as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ZerO_Init_on_matrix(matrix_tensor):\n",
    "    # Algorithm 1 in the paper.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    \n",
    "    if m <= n:\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    elif m > n:\n",
    "        clog_m = math.ceil(math.log2(m))\n",
    "        p = 2**(clog_m)\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m, p))\\\n",
    "                    @ (torch.tensor(hadamard(p)).float()/(2**(clog_m/2)))\\\n",
    "                    @ torch.nn.init.eye_(torch.empty(p, n))\\\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def Identity_Init_on_matrix(matrix_tensor):\n",
    "    # Definition 1 in the paper\n",
    "    # See https://pytorch.org/docs/stable/nn.init.html#torch.nn.init.eye_ for details. Preserves the identity of the inputs in Linear layers, where as many inputs are preserved as possible, the same as partial identity matrix.\n",
    "    \n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    print(\"Init matrix\", matrix_tensor.shape, \"with m={}, n={}\".format(m, n))\n",
    "    \n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m, n))\n",
    "    \n",
    "    return init_matrix\n",
    "\n",
    "def OnE_Init_on_matrix(matrix_tensor):\n",
    "    m = matrix_tensor.size(0)\n",
    "    n = matrix_tensor.size(1)\n",
    "    init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "    if m <= n:\n",
    "        print(\"Nothing to be done to OnE-Initialize\", init_matrix.shape);\n",
    "    elif m > n:\n",
    "        init_matrix = torch.nn.init.eye_(torch.empty(m,n))\n",
    "        rng = np.random.default_rng()\n",
    "        for row in range(n, m, 1):\n",
    "            col = rng.integers(low=0, high=n-1, endpoint=True)\n",
    "            print(\"Random column selected to be initialized to OnE (0 to {}): {}\".format(n-1, col))\n",
    "            init_matrix[row, col] = 1\n",
    "    else:\n",
    "        assert(False)\n",
    "    return init_matrix\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    a standard model with 4 hidden layers\n",
    "    '''\n",
    "    def __init__(self, n_h=1024, init='ZerO'):\n",
    "        super(MLP, self).__init__()\n",
    "        self.init = init\n",
    "        self.n_h = n_h\n",
    "        self.l1 = nn.Linear(784, 784, bias=False)  \n",
    "        self.l2 = nn.Linear(784, self.n_h, bias=False)  \n",
    "        self.l3 = nn.Linear(self.n_h, self.n_h, bias=False)  \n",
    "        self.l4 = nn.Linear(self.n_h, 10, bias=False)  \n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def forward(self, x): \n",
    "\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.l1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.l4(x)\n",
    "        return F.log_softmax(x)\n",
    "    \n",
    "    def _init_weights(self, m):\n",
    "        \n",
    "        if self.init == 'ZerO':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = ZerO_Init_on_matrix(m.weight.data)\n",
    "                \n",
    "        elif self.init == 'Partial_Identity':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = Identity_Init_on_matrix(m.weight.data)\n",
    "        \n",
    "        elif self.init == 'Random':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "        elif self.init == 'OnE':\n",
    "            if isinstance(m, nn.Linear):\n",
    "                m.weight.data = OnE_Init_on_matrix(m.weight.data)\n",
    "        else:\n",
    "            assert(False)\n",
    "                \n",
    "        if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rank(tensor):\n",
    "\n",
    "    tensor = tensor.detach().cpu()\n",
    "    rank = np.linalg.matrix_rank(tensor, tol=0.0001)\n",
    "    \n",
    "    return rank\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Pipeline on MNIST\n",
    "\n",
    "from https://github.com/pytorch/examples/blob/main/mnist/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification of Theorem 1 (Figure 3 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class Optimizer(torch.optim.SGD):\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                d_p = p.grad.data\n",
    "                p.data.add_(d_p, alpha=-group['lr']) \n",
    "\n",
    "        return loss\n",
    "\n",
    "def train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict):\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tAccuracy({:.0f}%)'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item(),\n",
    "                100. * correct / len(train_loader.dataset)))\n",
    "            \n",
    "            # log metric\n",
    "            train_acc_list.append(100. * correct / len(train_loader.dataset))\n",
    "            train_loss_list.append(loss.item())\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.grad is not None:\n",
    "                    if 'l3' in name:\n",
    "                        if name not in rank_list_dict:\n",
    "                            rank_list_dict[name] = []\n",
    "                            \n",
    "                        # compute stable rank of the residual component\n",
    "                        rank_list_dict[name].append(compute_rank(param.data - torch.eye(param.data.size(0)).to(param.data.device)))\n",
    "                \n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "    return 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "def train_model(model, file_dir=None):\n",
    "    # Training settings\n",
    "    parser = argparse.ArgumentParser(description='PyTorch MNIST Example')\n",
    "    parser.add_argument('--batch-size', type=int, default=64, metavar='N',\n",
    "                        help='input batch size for training (default: 64)')\n",
    "    parser.add_argument('--test-batch-size', type=int, default=1000, metavar='N',\n",
    "                        help='input batch size for testing (default: 1000)')\n",
    "    parser.add_argument('--epochs', type=int, default=1, metavar='N',\n",
    "                        help='number of epochs to train (default: 14)')\n",
    "    parser.add_argument('--lr', type=float, default=0.1, metavar='LR',\n",
    "                        help='learning rate (default: 1.0)')\n",
    "    parser.add_argument('--gamma', type=float, default=0.7, metavar='M',\n",
    "                        help='Learning rate step gamma (default: 0.7)')\n",
    "    parser.add_argument('--no-cuda', action='store_true', default=True,\n",
    "                        help='disables CUDA training')\n",
    "    parser.add_argument('--seed', type=int, default=1, metavar='S',\n",
    "                        help='random seed (default: 1)')\n",
    "    parser.add_argument('--log-interval', type=int, default=50, metavar='N',\n",
    "                        help='how many batches to wait before logging training status')\n",
    "\n",
    "    parser.add_argument('--save-model', action='store_true', default=False,\n",
    "                        help='For Saving the current Model')\n",
    "\n",
    "    parser.add_argument('--name', type=str, default='test')  \n",
    "    \n",
    "    parser.add_argument('--init', type=str, default='ZerO') \n",
    "\n",
    "    args = parser.parse_args([])\n",
    "    use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                       ])),\n",
    "        batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "    model = model.to(device)\n",
    "    # debug\n",
    "    print(\"v ===== Before training =====\")\n",
    "    for name, params in model.named_parameters():\n",
    "        print(params.shape)\n",
    "        print(name, params.data)\n",
    "    print(\"^ ===== Before training =====\")   \n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    " \n",
    "    # logging metric    \n",
    "    train_acc_list = []\n",
    "    train_loss_list = []\n",
    "    rank_list_dict = {}\n",
    "    \n",
    "    scheduler = StepLR(optimizer, step_size=12, gamma=args.gamma)\n",
    "    \n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        train(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\n",
    "        test(model, device, test_loader)\n",
    "        scheduler.step()\n",
    "    # debug2\n",
    "    print(\"v ===== After training =====\")\n",
    "    for name, params in model.named_parameters():\n",
    "        print(params.shape)\n",
    "        print(name, params.data)\n",
    "    print(\"^ ===== After training =====\")\n",
    "    optimizer = Optimizer(model.parameters(), lr=args.lr)\n",
    "    if args.save_model:\n",
    "        torch.save(model.state_dict(), args.init + \"_mnist_cnn.pt\")\n",
    "        \n",
    "    return rank_list_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (left): identity initialization under different widths\n",
    "\n",
    "We show that the rank constraints (training degeneracy) happen no matter what the width is. The ranks are always smaller than the input dimension (784=28 * 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([256, 784]) with m=256, n=784\n",
      "Init matrix torch.Size([256, 256]) with m=256, n=256\n",
      "Init matrix torch.Size([10, 256]) with m=10, n=256\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([256, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([256, 256])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 256])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10220/1659671277.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.973962\tAccuracy(2%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.614879\tAccuracy(6%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.626663\tAccuracy(11%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.337751\tAccuracy(15%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.646007\tAccuracy(20%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.255932\tAccuracy(25%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.715249\tAccuracy(29%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.408288\tAccuracy(34%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.407710\tAccuracy(39%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.141189\tAccuracy(44%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.129136\tAccuracy(49%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m n_h_256_rank_list_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPartial_Identity\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_h\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m n_h_512_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartial_Identity\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m))\n\u001b[1;32m      5\u001b[0m n_h_1024_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartial_Identity\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n",
      "Cell \u001b[0;32mIn[5], line 132\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, file_dir)\u001b[0m\n\u001b[1;32m    129\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m StepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, gamma\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, args\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m--> 132\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_acc_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrank_list_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     test(model, device, test_loader)\n\u001b[1;32m    134\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[5], line 28\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, train_acc_list, train_loss_list, rank_list_dict)\u001b[0m\n\u001b[1;32m     26\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 28\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m args\u001b[38;5;241m.\u001b[39mlog_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Epoch: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m [\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{:.6f}\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mAccuracy(\u001b[39m\u001b[38;5;132;01m{:.0f}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     31\u001b[0m         epoch, batch_idx \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(data), \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset),\n\u001b[1;32m     32\u001b[0m         \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m batch_idx \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader), loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     33\u001b[0m         \u001b[38;5;241m100.\u001b[39m \u001b[38;5;241m*\u001b[39m correct \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     68\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m, in \u001b[0;36mOptimizer.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m         d_p \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdata\n\u001b[0;32m---> 13\u001b[0m         \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_h_256_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=256))\n",
    "\n",
    "n_h_512_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=512))\n",
    "\n",
    "n_h_1024_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "\n",
    "n_h_2048_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=2048))\n",
    "\n",
    "n_h_2048_rank_list_dict_OnE = train_model(MLP(init='OnE', n_h=2048))\n",
    "\n",
    "# plotting\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, n_h_256_rank_list_dict['l3.weight'], label='n_h=256', linewidth='2')\n",
    "ax.plot(x_axis, n_h_512_rank_list_dict['l3.weight'], label='n_h=512', linewidth='2')\n",
    "ax.plot(x_axis, n_h_1024_rank_list_dict['l3.weight'], label='n_h=1024', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict['l3.weight'], label='n_h=2048', linewidth='2')\n",
    "ax.plot(x_axis, n_h_2048_rank_list_dict_One['l3.weight'], label='n_h=2048_OnE', linewidth='2')\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_left.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figure 3 (right): Hadamard transfrom breaks training degeneracy\n",
    "\n",
    "We show that when initializing dimension-increasing layer with Hadamard transform, the rank constraints (training degeneracy) not exsist any more. The rank can be greater than the input dimension during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[ 0.0201,  0.0503,  0.0700,  ...,  0.0056, -0.0191, -0.0746],\n",
      "        [ 0.0666, -0.0243,  0.0235,  ..., -0.0355, -0.0771,  0.0545],\n",
      "        [ 0.0409, -0.0016,  0.0712,  ..., -0.0156, -0.0044,  0.0105],\n",
      "        ...,\n",
      "        [ 0.0744, -0.0808,  0.0199,  ..., -0.0243, -0.0044,  0.0383],\n",
      "        [-0.0387,  0.0667, -0.0020,  ..., -0.0293, -0.0265, -0.0952],\n",
      "        [ 0.0540,  0.0250, -0.0414,  ..., -0.0684, -0.0407,  0.0343]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[-0.0168,  0.0218,  0.0190,  ..., -0.0111,  0.0377, -0.0083],\n",
      "        [-0.0052, -0.0441, -0.0399,  ..., -0.0411, -0.0212, -0.0735],\n",
      "        [ 0.0191, -0.0336,  0.0178,  ..., -0.0766,  0.0653,  0.0089],\n",
      "        ...,\n",
      "        [ 0.0280, -0.1188, -0.0371,  ..., -0.0215,  0.0161,  0.0026],\n",
      "        [ 0.0519, -0.0013, -0.0097,  ..., -0.0343,  0.0263,  0.0216],\n",
      "        [-0.0265,  0.0718,  0.0696,  ..., -0.0436,  0.1138,  0.0248]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[-1.6743e-02,  1.0081e-02, -8.8099e-02,  ...,  2.7471e-03,\n",
      "         -2.9532e-02, -4.7705e-03],\n",
      "        [-3.6494e-02,  9.4931e-03,  2.2656e-02,  ..., -2.8350e-02,\n",
      "          3.2992e-02, -1.0237e-02],\n",
      "        [ 1.3138e-01, -1.5157e-02,  7.0867e-02,  ..., -8.3613e-05,\n",
      "         -6.9518e-03, -3.6540e-03],\n",
      "        ...,\n",
      "        [-3.8594e-02, -8.3653e-02, -2.7728e-02,  ...,  4.4013e-02,\n",
      "         -3.0070e-02,  5.7402e-02],\n",
      "        [ 4.0196e-03, -2.5625e-02,  1.4508e-02,  ...,  3.9420e-02,\n",
      "         -1.0473e-01, -1.1434e-01],\n",
      "        [-1.9194e-02,  5.8001e-02, -5.2332e-02,  ..., -5.3641e-02,\n",
      "         -5.6861e-02,  2.2894e-02]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[-0.0324, -0.0252, -0.0033,  ..., -0.0356,  0.0581, -0.0279],\n",
      "        [ 0.0206,  0.0602, -0.0344,  ..., -0.0410,  0.0041,  0.0040],\n",
      "        [-0.0557,  0.0499,  0.0282,  ...,  0.0481, -0.0033, -0.0743],\n",
      "        ...,\n",
      "        [-0.0095, -0.0216, -0.0787,  ..., -0.0096, -0.0018, -0.0499],\n",
      "        [-0.0912,  0.0384, -0.0085,  ..., -0.0120, -0.0916,  0.0099],\n",
      "        [-0.0095, -0.0179,  0.0334,  ...,  0.0830, -0.0068, -0.0103]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.354125\tAccuracy(0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10220/1659671277.py:26: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.292689\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.298670\tAccuracy(8%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.394753\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.159087\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.394199\tAccuracy(23%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.112931\tAccuracy(28%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.523944\tAccuracy(33%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.212017\tAccuracy(38%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.237420\tAccuracy(43%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.082517\tAccuracy(48%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.088909\tAccuracy(53%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.225549\tAccuracy(58%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.039499\tAccuracy(63%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.039899\tAccuracy(68%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.078862\tAccuracy(74%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.110236\tAccuracy(79%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.236993\tAccuracy(84%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.051369\tAccuracy(89%)\n",
      "\n",
      "Test set: Average loss: 0.1541, Accuracy: 9511/10000 (95%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[ 0.0201,  0.0503,  0.0700,  ...,  0.0056, -0.0191, -0.0746],\n",
      "        [ 0.0666, -0.0243,  0.0235,  ..., -0.0355, -0.0771,  0.0545],\n",
      "        [ 0.0409, -0.0016,  0.0712,  ..., -0.0156, -0.0044,  0.0105],\n",
      "        ...,\n",
      "        [ 0.0744, -0.0808,  0.0199,  ..., -0.0243, -0.0044,  0.0383],\n",
      "        [-0.0387,  0.0667, -0.0020,  ..., -0.0293, -0.0265, -0.0952],\n",
      "        [ 0.0540,  0.0250, -0.0414,  ..., -0.0684, -0.0407,  0.0343]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[-0.0170,  0.0218,  0.0191,  ..., -0.0114,  0.0375, -0.0096],\n",
      "        [-0.0045, -0.0434, -0.0421,  ..., -0.0411, -0.0208, -0.0716],\n",
      "        [ 0.0186, -0.0346,  0.0078,  ..., -0.0766,  0.0647,  0.0068],\n",
      "        ...,\n",
      "        [ 0.0278, -0.1187, -0.0356,  ..., -0.0210,  0.0166,  0.0025],\n",
      "        [ 0.0519, -0.0021, -0.0075,  ..., -0.0346,  0.0250,  0.0207],\n",
      "        [-0.0268,  0.0731,  0.0697,  ..., -0.0429,  0.1123,  0.0263]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[-0.0164,  0.0108, -0.0874,  ...,  0.0016, -0.0339, -0.0034],\n",
      "        [-0.0365,  0.0094,  0.0293,  ..., -0.0284,  0.0371, -0.0093],\n",
      "        [ 0.1314, -0.0147,  0.0777,  ...,  0.0008, -0.0047,  0.0025],\n",
      "        ...,\n",
      "        [-0.0382, -0.0827, -0.0200,  ...,  0.0440, -0.0191,  0.0604],\n",
      "        [ 0.0041, -0.0254,  0.0166,  ...,  0.0400, -0.1028, -0.1143],\n",
      "        [-0.0192,  0.0566, -0.0593,  ..., -0.0532, -0.0612,  0.0282]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[-0.0597, -0.0025, -0.0184,  ..., -0.0584,  0.0475, -0.0485],\n",
      "        [ 0.0937,  0.0700, -0.0398,  ..., -0.1017,  0.0162,  0.0040],\n",
      "        [-0.0557,  0.0753,  0.0562,  ...,  0.1147,  0.0052, -0.1428],\n",
      "        ...,\n",
      "        [ 0.0398, -0.0187, -0.1163,  ..., -0.0490,  0.0116, -0.0759],\n",
      "        [-0.0871,  0.0349, -0.0120,  ...,  0.0620, -0.1044,  0.0274],\n",
      "        [ 0.0039, -0.0136,  0.0463,  ...,  0.0944, -0.0055,  0.0136]])\n",
      "^ ===== After training =====\n",
      "Init matrix torch.Size([784, 784]) with m=784, n=784\n",
      "Init matrix torch.Size([1024, 784]) with m=1024, n=784\n",
      "Init matrix torch.Size([1024, 1024]) with m=1024, n=1024\n",
      "Init matrix torch.Size([10, 1024]) with m=10, n=1024\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.302585\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.387518\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.401102\tAccuracy(9%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.541363\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.241735\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.560055\tAccuracy(23%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.180239\tAccuracy(27%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.678365\tAccuracy(32%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.327334\tAccuracy(37%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.341815\tAccuracy(42%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.138724\tAccuracy(47%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.099397\tAccuracy(52%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.284145\tAccuracy(57%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.101840\tAccuracy(62%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.110993\tAccuracy(67%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.112484\tAccuracy(72%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.192544\tAccuracy(77%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.324636\tAccuracy(82%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.087515\tAccuracy(87%)\n",
      "\n",
      "Test set: Average loss: 0.1659, Accuracy: 9507/10000 (95%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== After training =====\n",
      "v ===== Before training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[ 0.0312,  0.0312,  0.0312,  ...,  0.0312,  0.0312,  0.0312],\n",
      "        [ 0.0312, -0.0312,  0.0312,  ..., -0.0312,  0.0312, -0.0312],\n",
      "        [ 0.0312,  0.0312, -0.0312,  ...,  0.0312, -0.0312, -0.0312],\n",
      "        ...,\n",
      "        [ 0.0312, -0.0312,  0.0312,  ..., -0.0312,  0.0312, -0.0312],\n",
      "        [ 0.0312,  0.0312, -0.0312,  ...,  0.0312, -0.0312, -0.0312],\n",
      "        [ 0.0312, -0.0312, -0.0312,  ..., -0.0312, -0.0312,  0.0312]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "^ ===== Before training =====\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 3.204832\tAccuracy(0%)\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 0.313350\tAccuracy(4%)\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.319433\tAccuracy(9%)\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 0.501207\tAccuracy(13%)\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.189326\tAccuracy(18%)\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.520636\tAccuracy(23%)\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.158095\tAccuracy(28%)\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.586473\tAccuracy(33%)\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.260996\tAccuracy(38%)\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.329167\tAccuracy(43%)\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.112401\tAccuracy(48%)\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.080823\tAccuracy(53%)\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.228567\tAccuracy(58%)\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.055384\tAccuracy(63%)\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.092666\tAccuracy(68%)\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.069544\tAccuracy(73%)\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.169860\tAccuracy(78%)\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.292203\tAccuracy(83%)\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.069095\tAccuracy(88%)\n",
      "\n",
      "Test set: Average loss: 0.1502, Accuracy: 9544/10000 (95%)\n",
      "\n",
      "v ===== After training =====\n",
      "torch.Size([784, 784])\n",
      "l1.weight tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "torch.Size([1024, 784])\n",
      "l2.weight tensor([[ 0.0312,  0.0312,  0.0312,  ...,  0.0312,  0.0312,  0.0312],\n",
      "        [ 0.0312, -0.0312,  0.0312,  ..., -0.0312,  0.0312, -0.0312],\n",
      "        [ 0.0312,  0.0312, -0.0312,  ...,  0.0312, -0.0312, -0.0312],\n",
      "        ...,\n",
      "        [ 0.0312, -0.0312,  0.0312,  ..., -0.0312,  0.0312, -0.0312],\n",
      "        [ 0.0312,  0.0312, -0.0312,  ...,  0.0312, -0.0312, -0.0312],\n",
      "        [ 0.0312, -0.0312, -0.0312,  ..., -0.0312, -0.0312,  0.0312]])\n",
      "torch.Size([1024, 1024])\n",
      "l3.weight tensor([[ 1.0792e+00, -1.6253e-02, -7.2578e-02,  ..., -5.4269e-03,\n",
      "          2.8565e-05, -8.6293e-03],\n",
      "        [-1.6179e-01,  1.2150e+00,  3.7637e-03,  ...,  1.5216e-03,\n",
      "         -3.7685e-03,  3.0927e-03],\n",
      "        [ 7.1769e-02,  1.0744e-02,  1.1465e+00,  ..., -7.7749e-04,\n",
      "          2.9988e-03, -5.3003e-03],\n",
      "        ...,\n",
      "        [-6.8089e-04, -9.5977e-04, -2.6339e-04,  ...,  1.0002e+00,\n",
      "          2.0938e-04,  1.0082e-04],\n",
      "        [-1.1107e-03, -1.0278e-03,  9.3397e-04,  ...,  1.9351e-04,\n",
      "          1.0001e+00,  3.2419e-05],\n",
      "        [-2.3491e-03,  3.7793e-05, -1.4508e-03,  ...,  1.4941e-04,\n",
      "          6.2993e-05,  1.0000e+00]])\n",
      "torch.Size([10, 1024])\n",
      "l4.weight tensor([[ 1.2411e+00, -4.7199e-02, -1.0321e-01,  ..., -5.3784e-03,\n",
      "         -1.1988e-03, -8.4532e-03],\n",
      "        [-1.0908e-01,  1.3595e+00, -1.6572e-02,  ...,  1.5397e-03,\n",
      "         -1.8692e-03,  1.9083e-03],\n",
      "        [ 1.1326e-01, -1.6533e-02,  1.3304e+00,  ..., -1.6743e-03,\n",
      "          2.7040e-03, -6.3981e-03],\n",
      "        ...,\n",
      "        [-2.5509e-02,  9.3963e-02,  1.6305e-01,  ..., -2.2983e-03,\n",
      "          3.2898e-03, -4.1319e-03],\n",
      "        [-6.8902e-02, -1.0127e-01, -6.2287e-02,  ...,  1.6266e-02,\n",
      "          1.8535e-02,  6.6049e-04],\n",
      "        [ 1.7682e-02, -4.7501e-02, -2.5815e-01,  ...,  8.6056e-03,\n",
      "          2.8480e-03,  1.1957e-02]])\n",
      "^ ===== After training =====\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m partial_identity_init_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPartial_Identity\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n\u001b[1;32m      3\u001b[0m ZerO_init_rank_list_dict \u001b[38;5;241m=\u001b[39m train_model(MLP(init\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZerO\u001b[39m\u001b[38;5;124m'\u001b[39m, n_h\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m,\u001b[38;5;241m4\u001b[39m), gridspec_kw \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwspace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhspace\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0.5\u001b[39m})\n\u001b[1;32m      7\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(n_h_512_rank_list_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml3.weight\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m x_axis \u001b[38;5;241m=\u001b[39m x_axis \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "random_init_rank_list_dict = train_model(MLP(init='Random', n_h=1024))\n",
    "partial_identity_init_rank_list_dict = train_model(MLP(init='Partial_Identity', n_h=1024))\n",
    "ZerO_init_rank_list_dict = train_model(MLP(init='ZerO', n_h=1024))\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4), gridspec_kw = {'wspace':0.5, 'hspace':0.5})\n",
    "\n",
    "x_axis = np.arange(1, len(n_h_512_rank_list_dict['l3.weight'])+1)\n",
    "x_axis = x_axis * 50\n",
    "\n",
    "# generate a line of 784\n",
    "input_dim_line = np.ones(len(n_h_512_rank_list_dict['l3.weight'])) * 784\n",
    "\n",
    "ax.plot(x_axis, input_dim_line, label='input_dim=784', linestyle='dashed', color='red', linewidth='2')\n",
    "ax.plot(x_axis, random_init_rank_list_dict['l3.weight'], label='Random Init', linewidth='2')\n",
    "ax.plot(x_axis, partial_identity_init_rank_list_dict['l3.weight'], label='Partial Identity Init', linewidth='2')\n",
    "ax.plot(x_axis, ZerO_init_rank_list_dict['l3.weight'], label='ZerO Init', linewidth='2')\n",
    "\n",
    "\n",
    "ax.set_ylabel('Rank', fontsize=14)\n",
    "ax.set_xlabel('Iterations', fontsize=14) \n",
    "ax.legend(fontsize=12)\n",
    "\n",
    "fig.tight_layout(w_pad=0.5)\n",
    "plt.show()\n",
    "fig.savefig('./figure_3_right.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
